---
title: "index"
format: html
editor: 
  markdown: 
    wrap: 72
---

# Q1.

#### Take a moment to reflect on the value of open data:

How does easy access to historical and real-time environmental data
shape our understanding of climate trends, resource management, and
public health? What happens when this data disappears or becomes
inaccessible? The role of independent archiving and collaborative
stewardship has never been more critical in ensuring scientific progress
and accountability.

1.  When we can access historical and real-time data, we're better able
    to draw connections between previous conditions and the present to
    identify trends over time. We can see how present observations fit
    with previous data, which is the most important evidence that
    climate change is happening. When these data are accessible to
    everyone, we can hold our leaders accountable for the decisions they
    make, which are supposed to be on behalf of the public. Natural
    resource management is a very rough science that can be heavily
    influenced by policies, so access to data over time is crucial for
    holding agencies accountable in processes like the National
    Environmental Policy Act, where the public has a chance to influence
    agency actions, or the actions of any major actor on natural
    resources, or to identify trends over time that have multiple
    influences. When the data disappear or become inaccessible, people
    can get away with lying to support ulterior motives and decision
    makers can make choices with catastrophic consequences.

# Q2.Daily Summary

-   Doing fewer than 150 tests per 100,000 residents daily (over a 7-day
    average)

-   **More than 100 new cases per 100,000 residents over the past 14
    days… (focus on this)**

-   25 new cases per 100,000 residents and an 8% test positivity rate

-   10% or greater increase in COVID-19 hospitalized patients over the
    past 3 days

-   Fewer than 20% of ICU beds available

-   Fewer than 25% ventilators available

 For **More than 100 new cases per 100,000 residents over the past 14
    days**, report also:

1.  cumulative cases in the 5 worst counties
2.  total NEW cases in the 5 worst counties
3.  A list of safe counties
4.  A text report describing the total new cases,
5.  total cumulative cases, and number of safe counties.

### Steps

1.  Start by reading in the data from the NY-Times URL with read_csv
    (make sure to attach the tidyverse)
```{r, echo = TRUE}

library(dplyr)
library(tidyverse)
library(flextable)
library(zoo)
data = read_csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv')
```

2.  Create an object called my.date and set it as “2022-02-01” - ensure
    this is a date object:. Create a object called my.state and set it
    to “Colorado”.
    
```{r, echo = TRUE}
txt <- "2022-02-01"
class(txt)

my.date <- as.Date(txt)
class(my.date)

```

```{r, echo = TRUE}
my.state <- as.character("Colorado")
class(my.state)

```

3.The URL based data read from Github is considered our “raw data”.
Remember to always leave “raw-data-raw” and to generate meaningful
subsets as you go.

Start by making a subset that limits (filter) the data to Colorado and add a new column (mutate) with the daily new cases using diff/lag by county (group_by). Do the same for new deaths as well.

```{r, echo = TRUE}
co_covid <- data %>% 
  filter(state == "Colorado") %>% 
  group_by(county) %>% 
  mutate(case_lag = lag(cases, n = 1)) %>% 
  mutate(new_cases = cases-case_lag) %>% 
  mutate(death_lag = lag(deaths, n = 1)) %>%
  mutate(new_deaths = deaths - death_lag) %>% 
  arrange(date) %>% 
  ungroup()

```

  Q2. Using your subset, generate (2) tables. The first should show the 5 counties with the most CUMULATIVE cases, 
and the second should show the 5 counties with the most NEW cases. Remember to use your my.date
    object as a proxy for today’s date:

```{r, echo = TRUE}
#five counties with the most cumulative cases

most_cumulative_cases_co <- co_covid %>% 
   filter(date == my.date) %>% 
  select(county, cases) %>%
  slice_max(order_by = cases, n = 5)

library(knitr)
kable(most_cumulative_cases_co, caption = "Five Colorado counties with the most cumulative COVID cases as of February 1st, 2022")

```

```{r, echo = TRUE}
#five counties with the most new cases

most_new_cases_co <- co_covid %>% 
  filter(date == my.date) %>% 
  select(county, new_cases) %>% 
  slice_max(order_by = new_cases, n = 5, na_rm = TRUE)

kable(most_new_cases_co, caption = "Five Colorado counties with the most new COVID cases on Febrary 1st, 2022")
  
```

Your tables should have clear column names and descriptive captions.


# Q3: Normalizing Data

Raw count data can be deceiving given the wide range of populations in Colorado countries. To help us normalize data counts, we need supplemental population data to be added. Population data is offered by the Census and can be found here. Please read in this data.


How FIPS codes are structured
- The number of digits in a FIPS code depends on the level of geography
- State FIPS codes have two digits
- County FIPS codes have five digits, with the first two digits representing the state FIPS code


### 1. Convert the STATE numeric into a character forced to 2 digits with a leading 0 (when needed)

### 2. Convert the COUNTY numeric into a character forced to 3 digits with leading 0’s (when needed)

### 3. Create a FIP variable the STATE numeric into a character forced to 2 digits with a leading 0 (when needed)


```{r, echo = TRUE}
pop_url <- 'https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/totals/co-est2023-alldata.csv'

pop = read_csv(pop_url)

# In this part, we're adding the correct amount of digits to each component of the fips data, which in this table was separated. We want to merge the state and county column to get a complete county fip, which will be the overlapping we make our join table off of

pop <- pop |>
  mutate(STATE = as.numeric(STATE), COUNTY = as.numeric(COUNTY))|>
  mutate(fips = paste0(sprintf("%02d",STATE),sprintf("%03d", COUNTY)))
  

```


### 1. Given the above URL, and guidelines on string concatenation and formatting, read in the population data and (1) create a five digit FIP variable and only keep columns that contain “NAME” or “2021” (remember the tidyselect option found with ?dplyr::select). Additionally, remove all state level rows (e.g. COUNTY FIP == “000”)


1
```{r, echo = TRUE}
library(stringr)

pop21 <- pop %>%
  select(fips, contains("2021"), contains("NAME")) %>%
filter(!str_ends(fips, "000"))



```

### 2. Now, explore the data … what attributes does it have, what are the names of the columns? Do any match the COVID data we have? What are the dimensions… In a few sentences describe the data obtained after modification:
```{r, echo = TRUE}
names(pop21)
dim(pop21)
nrow(pop21)
str(pop21)
glimpse(pop21)
#my computer can't do skimr()

intersect(names(pop21), names(co_covid))

```
2. There are 3144 rows and 19 columns in pop21. There are demographic data like births, deaths, and migration. As of right now, the only columns that intersect are fips because I created that column in the population dataset. Both datasets have county names, state names. While pop21 has total deaths, co_covid just has COVID deaths.What I'm trying to figure out is how a join will happen if the number of rows for my population and covid data don't match up.


### 3 What is the range of populations seen in Colorado counties in 2021:
```{r, echo = TRUE}
COpop21 <- pop21 %>% 
filter(STNAME == "Colorado") %>%  mutate(state = STNAME, county = CTYNAME) %>% 
  select(-CTYNAME, -STNAME) %>%
  mutate(county = str_replace(county, " County", ""))

  

range(COpop21$POPESTIMATE2021)

```
3. In Colorado, county populations range from 741 to 737287.

### 4. Join the population data to the Colorado COVID data and compute the per capita cumulative cases, per capita new cases, and per capita new deaths:

```{r}
co_covid <- co_covid %>% 
  arrange(county)

CO_covid_pop <- left_join(co_covid, COpop21, by = NULL)

```

### 5. Generate (2) new tables. The first should show the 5 counties with the most cumulative cases per capita on 2021-01-01, and the second should show the 5 counties with the most NEW cases per capita on the same date. Your tables should have clear column names and descriptive captions.

#### First Table
```{r, echo = TRUE}

most_cumulative_percap <- CO_covid_pop %>%
  mutate(date_char = as.character(date)) %>% 
  mutate(cumulative_percap = cases / POPESTIMATE2021) %>% 
select(cumulative_percap, county, date_char, cases) %>% 
  group_by(date_char) %>%
  arrange(date_char) %>% 
  filter(date_char == "2021-01-01")  %>% slice_max(order_by = cumulative_percap, n = 5)

 most_cumulative_percap %>%
  flextable(col_keys = c("cumulative_percap", "cases", "county")) %>%  # 
  width(width = 0.75) %>%
  height(height = 0.25) %>%
  set_header_labels(
    cases = "Cumulative Cases",
    cumulative_percap = "Cases per Capita",
    county = "County"
  ) %>%  
   set_caption("Top 5 Counties by Cumulative COVID Cases per Capita on Jan 1, 2021") %>% 
  theme_booktabs() 

```
#### Second Table
```{r, echo = TRUE}
most_new_percap <- CO_covid_pop %>%
  mutate(date_char = as.character(date)) %>% 
  mutate(new_percap = new_cases / POPESTIMATE2021) %>% 
select(new_percap, county, date_char, new_cases, cases) %>% 
  group_by(date_char) %>%
  arrange(date_char) %>% 
  filter(date_char == "2021-01-01")  %>% slice_max(order_by = new_percap, n = 5)

 most_new_percap %>%
  flextable(col_keys = c("new_percap", "new_cases", "county")) %>%
  width(width = 0.75) %>%
  height(height = 0.25) %>%
  set_header_labels(
    new_percap = "New Cases per Capita",
    new_cases = "New Cases",
    county = "County"
  ) %>%  
   set_caption("Top 5 Counties by New COVID Cases per Capita on Jan 1, 2021") %>% 
  theme_booktabs()

```

Q4. Rolling Thresholds

Filter the merged COVID/Population data to only include the last 14 days. Remember this should be a programmatic request and not hard-coded. Then, use the group_by/summarize paradigm to determine the total number of new cases in the last 14 days per 100,000 people. Print a table of the top 5 counties, and, report the number that meet the watch list condition: “More than 100 new cases per 100,000 residents over the past 14 days…”

```{r, echo = TRUE}

data <- data %>% 
group_by(county, date) %>% 
arrange(county) %>% 
 mutate(lag_cases = lag(cases, n = 1))


covid_pop <- 

last_14_new <- co_covid_pop


```